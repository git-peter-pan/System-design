what is system design?
process of define data, algorithm, module, architecture for a system to satisfy some specific requirements.
process of defining data, interface, module, component, architecture for a system to satisfy specific requirements.

What is a good design?
healthiness
every part execute well, comunicate well
simplicity: no more no less.

http://www.aosabook.org/en/distsys.html

cdn: content delivery network. 
a golobally distributed network of server to server the content(like, image, video);
serve content to end-users with high availability and high performance.

Service Oriented Architecture: each service has its own functional context, different services 
communicate with abstract interface.

problem of first solution
1. upload and download compete resource each other.
download:upload is usually 3:1
2.read files from cache, write to disk
database write is alwasy slower than reads.

another problem
1.server has a upper limit of simultaneous connection(500 default, can much higher)
in high trffic, write consumes all.

read can be ashnchrnous, or compression or chunk transfer(batch).
so server can handle many more qps than the maximum.(serveral k, if maximum connection is 500)
write, hold the connection during uploading. upload 1-2mb/s, so can only handle 500.


solution: split out reads and writes of images
advantage: 
1.scale each of them independently.
2.clarify what is going on at each point easier.
3.easier to troubleshooot

how to speed up: 
1.queue up the requests.
2.cache the popular images.

flickr solve the read/write problem:
distribute users across different shards.
users increase, add more shards.

Problem: one of the services brings down(for example, one one can write files)
whereas flicker will only affect those specific users.

shard problem:
update, first example, only need update one write service.
flicker needs to update each shards metadata for searching.

Redundancy: in order to solve failure(losing data, or a service is down);

backup database or spare service.
once one of them are down, the system can failover to the healthy copy.

problem: large data sets. so that single server cannot handle that.
solution: scale vertically/horizonally
vertically means add more resource to an individual server. faster cpu, more memory, etc

horizonally:add more nodes. store part of data in other data set. splitting opeartions to other nodes.
means partition or shards.
shared-nonthing: 


problem of doing that: 
1. data locality. have data spread through multiple servers, cost more time for fetching data across network.
2.data consistency.

LAMP: a typical model for building web service.
Linux
Apache HTTP server
MySQL relational database management system
Php language.

Problem of LAMP as they grow:
scalability of data layer.
fast access of data.
usually app layer is minimized and is share-nothing system(loose- coupled);


8 bits = 1 byte
1000 byte = 1kB

problem: assume you have TB level data, and user want to access small portion of data
it's very costly to load TB data into memory, so it directly tranlates to disk LEVEL. which 
is soso much slower than memory.
memory speed: chuck norris
disk access: line at DMV
1 ns = 10^-9 s

read 1m sequencially in main memory: 250 us
read 1m sequencially in disk 20ms 80times memory
even faster when you random access the memory. 100k times of disk
 http://queue.acm.org/detail.cfm?id=1563874

4 solutions:
cache
proxies,
indexes,
load banlancers.

cache:
like short term memory
contain the recently acessed data.

where to place:
directly on a request layer node: enable the local storage of data.
request will check if the data is in the local cache, if yes, return the data without querying 
the data center.

What happen to many node?
Have each host its own cache.

technology diff:
memory : DRAM, dynamic random access memory
cache: SRAM, static randon acess memory
problem: 
if you load balancer distributed randomly the request, the same request will go to another node.
It will miss the cache.

solution:
1. global cache
2.distributed cache

global cache: 
use single cache for different nodes.
problem: easy to overwhelm/full.

two forms:
(use more)1,request to cache, if no. cache to datacenter, and add data into cache from datacenter
why: datacenter prevent large numbers of request from clients.
2.request cache, if no, request to datacenter. and datacenter to cache to add data.
sometimes:

distributed cache:
use a hashing to find the cache.

problem: missing node, add/remove node, get complicated quickly.

memcached(http://memcached.org/) 
key-value store, lookup O(1);


Proxy
a proxy server is an intermediate piece of hardware that receives request from clients and modified 
them and relay them to origin servers.
used to filter requests, log requests, transform requests.

1proxy collapse serveral requests for same data into one request.
2collopase serveral request for close data into on request.


you can use proxies and caches together,
generally put cache in front of proxy.
reason: cache is faster, doesn't mid multiple request for same data.
if cache is behind proxy, get some latency.

name some proxy:  Squid and Varnish


indexes:
a table map data to specific location

inverted index:


load balancers:
distribute load across a set of node
advantage:
can handle a lot of simultanously connection and route these connection to one of requests node
2.easy to scale, just add nodes

algorithm to pick node:
round robin
based on some criteria(cpu or memory utilization)

name some load balancer:
HAProxy

problem: 
user-session-specific data.
store data in a session, when visit next time, if go to another node, that session may lose.
solution: cookies

browser cache

Queue:
advan: can handle task asynchrounously.
users know the taks are acceptable properly. the server will send the acknowledge later about 
task result. 


name some software implementing queue
redis.



 gfs save:
 disk: metadata + data
 unit: block 1024bytes


 p:file become bigge, metedata become bigger
 s:bigger unit chunck 64mb;

 p:extra large file
 master to store metadata.(index)
 chunkserver to store data.

 problem: chunkserver need to notify master the change, consume all the bandwidth
 sol:index more general, for which chunkserver instead of which place in that chunkserver.
 chunkserver has his own index

 problem: how to check one chunk is down
 sol: one chunk has a list of blocks.
 1 block = 64kb;
 1 block has a check sum(32bit)
 1T/64kb*32 = 64MB;
 it will compare its checksum when it reads a block.

 problem: what if one chunk is down
 sol:copy 2 more.

 how to choose which 2:
 1.disk utilization
 2.limit number of recent 
 3.across rack 2 + 1; two data center, two racks.

 problem:how to recover?
 ask master, where the other,
 master frop broken chunkserver in index.
 get the info back.
 find the closest one,
 get the copy back. 
 notify master to change index!!

 problem: chunkserver is down
 chunkserver sent heartbest to master
 1min/time, wait like 5 min(since maybe network problem);
 if heartbeat timeout, know the chunkserver is broken.

 problem: how to recover.
 repair proceduce
 working list
 chunk99, cs1
 chunk03, cs3,cs5

 the less, the high priority

 problem: how to aoid hot spot;
 make more copies of these hot resources.


 gfs read:
 1.client to master
 request: filename, chunk index
 2.master to client
 find table in a file namespace
 chunk handle, chunk location
 find index table 
 3. client to chunkserver(closest one)
 chunk handle, byte range
 4.chunkserver to data
 get data.

problem: is master can handle
ans: master is easy enought, can handle 10^9


fgs:write file
7steps in total:
step 3: why cache first: 1.fast 2 avoid inconsistency


static page:
can not update automatically. basically, .html page.
dynamic page:
can update content through backend, post news, product info.


http://www.cnblogs.com/sharpxiajun/p/4237704.html
web architecture:

hardware part:

bandwidth: 
active daily user: 1m
average qps = 1m/864000 = 12
average page = 100kb;
average bandwidth = 12*100kb*8 = 9600kbit = 9.6Mbps
peak = average*5 = 45Mbps;

my prob: how request find which server need to serve?
server:
image, page, data,log
data is the most important, usually is the bottleneck.
usually use master-slave to seperate the read and write.

horizonal:diff table in diff servers, prob: join
vertical:diff id in diff servers, prob: count, sum();

memcache: a key/value hashmap, improve the read performance.

software part:

representation/logic/??/persistance

usually deploy wep app in two servers. 
prob: need to synchronized the user's state.
sol: session
servers need to send the session between each other.

prob: server increase, the ability to handle qps doesn't increase linear.
sol:memcache/ for session in a seperate server. why:map sessionid to specific server
prob: when that server is down, cannot serve that client.
easier way, store the session in cookie.


session vs cookie
session: server side file
cookie: client side file

HTTP:hypert text transfer protocol
HTTPS: hyper text transfer protocol over SSL; use to authenticate and payment transaction

different website has different feature:
paypal, write > read
facebook, read > write;
amazon: read > write; 4:1??

website is down when high qps
90% is because database, bottle neck in store.
for read >> write, sol: seperate write and read.

master for write.
slave for read.
master usually is ok, do not need so many writes.
batch operations for synchronization.
if cannot handle read. sol: cache some not frequently changed data.
prob: real time update.
reason: when request comes, return cache first(usuallly the old data);


facebook: batch operations.

above that, solve database down problem.

而读写分离方案主要是应用于网站读写比例严重失衡的网站

qps get higher:
export data from database to file, create index.
search usually use LIKE, like is inefficient in database o(N);
LIKE  IS FASTER IN search?? why

data become more and more until one server cannot handle all data.
vertical sperate: 20% big table. 
seperate the bigest table, like comodity_table, transaction_table

data more and more untial one server cannot handle one table.
horizonal seperate: seperate one table into different servers.

system design principle: simple, no more, no less.

verizonal seperate: 
prob:how to set primary key
sol
1.set step size.
for example,if seperate into 2 parts.
1,3,5,7,9
2,4,6,8,10 won't conflict

2.estimate size and set start point.

if not use auto increament, use other customize rules.
hash and get the mod.


12306网站
http://coolshell.cn/articles/6470.html
秒杀新的解决方案
1.only handle first n requests.
2.log first n request.
when finish sec-kill,
batch all the request to modify the database.
alibaba sol: input validata code, cdn will filter all the other users.
so only n request can go to database.
usually asynchronized the write operation, put them into queue.


















